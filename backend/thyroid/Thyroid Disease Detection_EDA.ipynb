{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\python311\\lib\\site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python311\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\python311\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\python311\\lib\\site-packages (from imbalanced-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\python311\\lib\\site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\python311\\lib\\site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\python311\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from imbalanced-learn) (3.1.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\python311\\lib\\site-packages (0.23.1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\python311\\lib\\site-packages (from mlxtend) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\python311\\lib\\site-packages (from mlxtend) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\python311\\lib\\site-packages (from mlxtend) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\python311\\lib\\site-packages (from mlxtend) (1.2.2)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\python311\\lib\\site-packages (from mlxtend) (3.7.1)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\python311\\lib\\site-packages (from mlxtend) (1.2.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=0.24.2->mlxtend) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\python311\\lib\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.1.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas-profiling\n",
      "  Downloading pandas_profiling-3.2.0-py2.py3-none-any.whl (262 kB)\n",
      "                                              0.0/262.6 kB ? eta -:--:--\n",
      "     -------------                           92.2/262.6 kB 1.7 MB/s eta 0:00:01\n",
      "     -------------------------------------  256.0/262.6 kB 3.1 MB/s eta 0:00:01\n",
      "     -------------------------------------- 262.6/262.6 kB 2.3 MB/s eta 0:00:00\n",
      "Collecting joblib~=1.1.0 (from pandas-profiling)\n",
      "  Downloading joblib-1.1.1-py2.py3-none-any.whl (309 kB)\n",
      "                                              0.0/309.8 kB ? eta -:--:--\n",
      "     -----------------                      143.4/309.8 kB 8.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- 309.8/309.8 kB 4.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\python311\\lib\\site-packages (from pandas-profiling) (1.10.1)\n",
      "Requirement already satisfied: pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3 in c:\\python311\\lib\\site-packages (from pandas-profiling) (1.5.3)\n",
      "Requirement already satisfied: matplotlib>=3.2.0 in c:\\python311\\lib\\site-packages (from pandas-profiling) (3.7.1)\n",
      "Requirement already satisfied: pydantic>=1.8.1 in c:\\python311\\lib\\site-packages (from pandas-profiling) (1.10.6)\n",
      "Requirement already satisfied: PyYAML>=5.0.0 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas-profiling) (6.0)\n",
      "Requirement already satisfied: jinja2>=2.11.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas-profiling) (3.1.2)\n",
      "Requirement already satisfied: markupsafe~=2.1.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas-profiling) (2.1.2)\n",
      "Collecting visions[type_image_path]==0.7.4 (from pandas-profiling)\n",
      "  Downloading visions-0.7.4-py3-none-any.whl (102 kB)\n",
      "                                              0.0/102.4 kB ? eta -:--:--\n",
      "     ---------------------------------------- 102.4/102.4 kB ? eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\python311\\lib\\site-packages (from pandas-profiling) (1.23.5)\n",
      "Collecting htmlmin>=0.1.12 (from pandas-profiling)\n",
      "  Downloading htmlmin-0.1.12.tar.gz (19 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting missingno>=0.4.2 (from pandas-profiling)\n",
      "  Downloading missingno-0.5.2-py3-none-any.whl (8.7 kB)\n",
      "Collecting phik>=0.11.1 (from pandas-profiling)\n",
      "  Downloading phik-0.12.4-cp311-cp311-win_amd64.whl (667 kB)\n",
      "                                              0.0/667.1 kB ? eta -:--:--\n",
      "     ----------------------------          512.0/667.1 kB 16.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- 667.1/667.1 kB 10.6 MB/s eta 0:00:00\n",
      "Collecting tangled-up-in-unicode==0.2.0 (from pandas-profiling)\n",
      "  Downloading tangled_up_in_unicode-0.2.0-py3-none-any.whl (4.7 MB)\n",
      "                                              0.0/4.7 MB ? eta -:--:--\n",
      "     ---                                      0.4/4.7 MB 13.5 MB/s eta 0:00:01\n",
      "     -------                                  0.9/4.7 MB 9.3 MB/s eta 0:00:01\n",
      "     -----------                              1.4/4.7 MB 9.9 MB/s eta 0:00:01\n",
      "     ----------------                         1.9/4.7 MB 10.0 MB/s eta 0:00:01\n",
      "     -------------------                      2.3/4.7 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------                   2.7/4.7 MB 10.6 MB/s eta 0:00:01\n",
      "     --------------------------               3.2/4.7 MB 10.6 MB/s eta 0:00:01\n",
      "     -------------------------------          3.7/4.7 MB 10.7 MB/s eta 0:00:01\n",
      "     -----------------------------------      4.2/4.7 MB 10.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.7/4.7 MB 10.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.7/4.7 MB 10.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.24.0 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas-profiling) (2.28.2)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in c:\\python311\\lib\\site-packages (from pandas-profiling) (4.65.0)\n",
      "Requirement already satisfied: seaborn>=0.10.1 in c:\\python311\\lib\\site-packages (from pandas-profiling) (0.12.2)\n",
      "Collecting multimethod>=1.4 (from pandas-profiling)\n",
      "  Downloading multimethod-1.10-py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied: attrs>=19.3.0 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (22.2.0)\n",
      "Requirement already satisfied: networkx>=2.4 in c:\\python311\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (3.1)\n",
      "Collecting imagehash (from visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Downloading ImageHash-4.3.1-py2.py3-none-any.whl (296 kB)\n",
      "                                              0.0/296.5 kB ? eta -:--:--\n",
      "     -------------------------------------- 296.5/296.5 kB 9.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: Pillow in c:\\python311\\lib\\site-packages (from visions[type_image_path]==0.7.4->pandas-profiling) (9.4.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (4.39.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (23.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\python311\\lib\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib>=3.2.0->pandas-profiling) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas!=1.0.0,!=1.0.1,!=1.0.2,!=1.1.0,>=0.25.3->pandas-profiling) (2022.7.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\python311\\lib\\site-packages (from pydantic>=1.8.1->pandas-profiling) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.24.0->pandas-profiling) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.24.0->pandas-profiling) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.24.0->pandas-profiling) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from requests>=2.24.0->pandas-profiling) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.48.2->pandas-profiling) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib>=3.2.0->pandas-profiling) (1.16.0)\n",
      "Collecting PyWavelets (from imagehash->visions[type_image_path]==0.7.4->pandas-profiling)\n",
      "  Downloading pywavelets-1.5.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "                                              0.0/4.3 MB ? eta -:--:--\n",
      "     ----                                     0.4/4.3 MB 13.4 MB/s eta 0:00:01\n",
      "     ---------                                1.0/4.3 MB 12.2 MB/s eta 0:00:01\n",
      "     --------------                           1.5/4.3 MB 12.2 MB/s eta 0:00:01\n",
      "     --------------------                     2.2/4.3 MB 11.8 MB/s eta 0:00:01\n",
      "     -------------------------                2.8/4.3 MB 11.7 MB/s eta 0:00:01\n",
      "     -------------------------------          3.3/4.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ------------------------------------     3.8/4.3 MB 11.7 MB/s eta 0:00:01\n",
      "     ---------------------------------------  4.2/4.3 MB 11.8 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.3/4.3 MB 10.9 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: htmlmin\n",
      "  Building wheel for htmlmin (setup.py): started\n",
      "  Building wheel for htmlmin (setup.py): finished with status 'done'\n",
      "  Created wheel for htmlmin: filename=htmlmin-0.1.12-py3-none-any.whl size=27092 sha256=6cb846b8af7935dd0cf2bccdf6475134ebeadee253c523d9e43ed9ba090c3da9\n",
      "  Stored in directory: c:\\users\\kajal\\appdata\\local\\pip\\cache\\wheels\\8d\\55\\1a\\19cd535375ed1ede0c996405ebffe34b196d78e2d9545723a2\n",
      "Successfully built htmlmin\n",
      "Installing collected packages: htmlmin, tangled-up-in-unicode, PyWavelets, multimethod, joblib, imagehash, visions, phik, missingno, pandas-profiling\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.2.0\n",
      "    Uninstalling joblib-1.2.0:\n",
      "      Successfully uninstalled joblib-1.2.0\n",
      "Successfully installed PyWavelets-1.5.0 htmlmin-0.1.12 imagehash-4.3.1 joblib-1.1.1 missingno-0.5.2 multimethod-1.10 pandas-profiling-3.2.0 phik-0.12.4 tangled-up-in-unicode-0.2.0 visions-0.7.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n",
    "!pip install imbalanced-learn\n",
    "!pip install mlxtend\n",
    "!pip install pandas-profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.0-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Collecting numpy<2,>=1.23.2 (from pandas)\n",
      "  Using cached numpy-1.26.3-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Collecting python-dateutil>=2.8.2 (from pandas)\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2023.4-py2.py3-none-any.whl (346 kB)\n",
      "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: pytz, tzdata, six, numpy, python-dateutil, pandas\n",
      "Successfully installed numpy-1.26.3 pandas-2.2.0 python-dateutil-2.8.2 pytz-2023.3.post1 six-1.16.0 tzdata-2023.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "streamlit 1.20.0 requires pandas<2,>=0.25, but you have pandas 2.2.0 which is incompatible.\n",
      "tensorflow-intel 2.12.0rc1 requires numpy<1.24,>=1.22, but you have numpy 1.26.3 which is incompatible.\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.5.3\n",
      "  Using cached pandas-1.5.3-cp311-cp311-win_amd64.whl (10.3 MB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas==1.5.3) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas==1.5.3) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from pandas==1.5.3) (1.26.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kajal\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
      "Installing collected packages: pandas\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 2.2.0\n",
      "    Uninstalling pandas-2.2.0:\n",
      "      Successfully uninstalled pandas-2.2.0\n",
      "Successfully installed pandas-1.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataError' from 'pandas.core.base' (C:\\Python311\\Lib\\site-packages\\pandas\\core\\base.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtree\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mneighbors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m KNeighborsClassifier\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m     21\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas_profiling\\__init__.py:6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Main module of pandas-profiling.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m.. include:: ../../README.md\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontroller\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pandas_decorator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas_profiling\\controller\\pandas_decorator.py:4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This file add the decorator on the DataFrame object.\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataFrame\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprofile_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ProfileReport\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprofile_report\u001b[39m(df: DataFrame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ProfileReport:\n\u001b[0;32m      8\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Profile a DataFrame.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m        A ProfileReport of the DataFrame.\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas_profiling\\profile_report.py:15\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, Settings\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexpectations_report\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExpectationsReport\n\u001b[1;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malerts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlertType\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdescribe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m describe \u001b[38;5;28;01mas\u001b[39;00m describe_df\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sample\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas_profiling\\model\\alerts.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorrelations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m perform_check_correlation\n\u001b[0;32m     13\u001b[0m \u001b[38;5;129m@unique\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mAlertType\u001b[39;00m(Enum):\n\u001b[0;32m     15\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Alert types\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Python311\\Lib\\site-packages\\pandas_profiling\\model\\correlations.py:8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultimethod\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m multimethod\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataError\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas_profiling\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCorrelation\u001b[39;00m:\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DataError' from 'pandas.core.base' (C:\\Python311\\Lib\\site-packages\\pandas\\core\\base.py)"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --target=%APPDATA%\\Python\\Python311\\site-packages pandas\n",
    "!pip install --upgrade pandas==1.5.3\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names=['age','sex','on_thyroxine','query_on_thyroxine','on_antithyroid_medication',\n",
    "       'sick','pregnant','thyroid_surgery','I131_treatment','query_hypothyroid','query_hyperthyroid',\n",
    "      'lithium','goitre','tumor','hypopituitary','psych','TSH_measured','TSH','T3_measured','T3','TT4_measured','TT4','T4U_measured',\n",
    "      'T4U','FTI_measured','FTI','TBG_measured','TBG','referral_source','classes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('allhypo.data',names=names,na_values='?')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split classes column\n",
    "col_mod=df.classes.str.split('.',expand=True)\n",
    "df[['last','final']]=col_mod\n",
    "data=df.drop(['classes','final'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=data.rename(columns={'last':'classes'})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Dataframe to csv file\n",
    "df.to_csv('Thyroid_Data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing values handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Null values in Dataset\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of null values in dataset\n",
    "df.isnull().sum()/df.shape[0]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TBG having 100% null values so will drop that column\n",
    "df=df.drop('TBG',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#age,sex,TSH,T3,TT4,T4U,FTI having null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features=df.select_dtypes(exclude='number')\n",
    "categorical_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking all the unique values inside categorical features\n",
    "for feature in categorical_features:\n",
    "    print('---------------------------------')\n",
    "    print(f\"{feature}:{categorical_features[feature].unique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for outliers\n",
    "\n",
    "fig, axes = plt.subplots(3,2, figsize=(18, 10))\n",
    "  \n",
    "\n",
    "sns.boxplot(ax=axes[0, 0], data=df, x='classes', y='age')\n",
    "sns.boxplot(ax=axes[0, 1], data=df, x='classes', y='TSH')\n",
    "sns.boxplot(ax=axes[1, 0], data=df, x='classes', y='T3')\n",
    "sns.boxplot(ax=axes[1, 1], data=df, x='classes', y='TT4')\n",
    "sns.boxplot(ax=axes[2, 0], data=df, x='classes', y='T4U')\n",
    "sns.boxplot(ax=axes[2, 1], data=df, x='classes', y='FTI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are outliers in dataset so will use median to fill null values in numerical and mode for categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "#Handle numerical features\n",
    "simple_imputer=SimpleImputer(strategy='median')\n",
    "numerical_missing=pd.DataFrame(simple_imputer.fit_transform(df.select_dtypes(exclude='O')))\n",
    "\n",
    "#Handle categorical features\n",
    "cat_imputation=SimpleImputer(strategy='most_frequent')\n",
    "categorical_missing=pd.DataFrame(cat_imputation.fit_transform(df.select_dtypes(exclude='number')))\n",
    "\n",
    "numerical_missing.columns=df.select_dtypes(exclude='O').columns\n",
    "categorical_missing.columns=df.select_dtypes(exclude='number').columns\n",
    "\n",
    "data=pd.concat([numerical_missing,categorical_missing],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outliers detection and removal\n",
    "\n",
    "Since all the numerical features not following Normal distribution, will use percentile method to detect the outliers and remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data.age>100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(data.age.index[1364])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers_removal(numerical_missing):\n",
    "    for column in numerical_missing:\n",
    "        sort=np.sort(numerical_missing[column])\n",
    "        lower_limit,upper_limit=np.percentile(sort,[0,95])\n",
    "        detected_outliers=numerical_missing.iloc[np.where((numerical_missing[column]>upper_limit) | (numerical_missing[column]<lower_limit))]\n",
    "        return detected_outliers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_data=outliers_removal(data)\n",
    "outliers_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=data.drop(outliers_data.index)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('Preprocessed_data.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = new_df.iloc[: , 0:-1]\n",
    "Y = new_df.iloc[: , -1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=1)\n",
    "X_train.shape,X_test.shape,Y_train.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Independent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_encoder = OrdinalEncoder()\n",
    "X_train_cat_encoded = pd.DataFrame(ordinal_encoder.fit_transform(X_train.select_dtypes(exclude='number')))\n",
    "X_train_cat_encoded.columns = X_train.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_cat_encoded = pd.DataFrame(ordinal_encoder.transform(X_test.select_dtypes(exclude='number')))\n",
    "X_test_cat_encoded.columns = X_test.select_dtypes(exclude='number').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the Dependent Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y_train_cat_encoded= pd.DataFrame(label_encoder.fit_transform(Y_train))\n",
    "print(Y_train_cat_encoded.value_counts())\n",
    "print(Y_train.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_cat_encoded = pd.DataFrame(label_encoder.transform(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "\n",
    "X_train_sc=pd.DataFrame(sc.fit_transform(X_train.select_dtypes(exclude='O')))\n",
    "X_test_sc=pd.DataFrame(sc.transform(X_test.select_dtypes(exclude='O')))\n",
    "\n",
    "X_train_sc.columns=X_train.select_dtypes(exclude='O').columns\n",
    "X_test_sc.columns=X_test.select_dtypes(exclude='O').columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_final=pd.concat([X_train_sc,X_train_cat_encoded],axis=1)\n",
    "\n",
    "X_test_final=pd.concat([X_test_sc,X_test_cat_encoded],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle imbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['classes'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since the dataset is small, will use over-sampling: SMOTE technique to balance the data\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique) consists of synthesizing elements for the minority class, based on those that already exist. It works randomly picking a point from the minority class and computing the k-nearest neighbours for this point. The synthetic points are added between the chosen point and its neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resample,Y_train_resample=SMOTE(random_state=0,k_neighbors=1).fit_resample(X_train_final,Y_train_cat_encoded)\n",
    "X_test_resample,Y_test_resample=SMOTE(random_state=0,k_neighbors=1).fit_resample(X_test_final,Y_test_cat_encoded)\n",
    "\n",
    "X_train_resample.shape,X_test_resample.shape,Y_train_resample.shape,Y_test_resample.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training dataset shape:', X_train_resample.shape, Y_train_resample.shape)\n",
    "print('Testing dataset shape:', X_test_resample.shape, Y_test_resample.shape)\n",
    "\n",
    "Y_train_resample_flat = Y_train_resample.to_numpy().ravel()\n",
    "Y_test_resample_flat = Y_test_resample.to_numpy().ravel()\n",
    "\n",
    "print('Training dataset shape:', X_train_resample.shape, Y_train_resample_flat.shape)\n",
    "print('Testing dataset shape:', X_test_resample.shape, Y_test_resample_flat.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5)\n",
    "\n",
    "forward_fs = sfs(rf , k_features=10,forward=True,floating=False,verbose=2,scoring='accuracy',cv=5)\n",
    "\n",
    "forward_fs = forward_fs.fit(X_train_resample, Y_train_resample_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = list(forward_fs.k_feature_names_)\n",
    "print(feat_names)\n",
    "\n",
    "X_train_new=X_train_resample[['age','sex','TSH', 'TT4', 'FTI', 'on_thyroxine', 'on_antithyroid_medication', 'goitre', 'hypopituitary', 'psych', 'T3_measured', 'referral_source']]\n",
    "X_test_new=X_test_resample[['age','sex','TSH', 'TT4', 'FTI', 'on_thyroxine', 'on_antithyroid_medication', 'goitre', 'hypopituitary', 'psych', 'T3_measured', 'referral_source']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model=rf.fit(X_train_new,Y_train_resample_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_Score(clf,x_train,x_test,y_train,y_test,train=True):\n",
    "    if train:\n",
    "        pred=clf.predict(x_train)\n",
    "        clf_report=pd.DataFrame(classification_report(y_train,pred,output_dict=True))\n",
    "        print(\"Train Result:\\n===============\")\n",
    "        print(f\"Accuracy Score:{accuracy_score(y_train,pred)*100:.2f}%\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(f\"Classification Report:\\n{clf_report}\")\n",
    "        print(\"-----------------------------------\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(y_train,pred)}\\n\")\n",
    "    elif train==False:\n",
    "        pred=clf.predict(x_test)\n",
    "        clf_report=pd.DataFrame(classification_report(y_test,pred,output_dict=True))\n",
    "        print(\"Test Result:\\n===============\")\n",
    "        print(f\"Accuracy Score:{accuracy_score(y_test,pred)*100:.2f}%\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(f\"Classification Report:\\n{clf_report}\")\n",
    "        print(\"---------------------------------\")\n",
    "        print(f\"Confusion Matrix:\\n{confusion_matrix(y_test,pred)}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Score(rf_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_Score(rf_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "lr=LogisticRegression(random_state=0,max_iter=10)\n",
    "\n",
    "lr_model=lr.fit(X_train_new,Y_train_resample_flat)\n",
    "\n",
    "lr_train_score=print_Score(lr_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)\n",
    "lr_test_score=print_Score(lr_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Navie Bayes Classification\n",
    "\n",
    "gnb=GaussianNB()\n",
    "\n",
    "gnb_model=gnb.fit(X_train_new,Y_train_resample_flat)\n",
    "\n",
    "gnb_train_score=print_Score(gnb_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)\n",
    "gnb_test_score=print_Score(gnb_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decision Tree\n",
    "\n",
    "dtc=DecisionTreeClassifier(random_state=0,max_depth=10,min_samples_split=5)\n",
    "dt_model=dtc.fit(X_train_new,Y_train_resample_flat)\n",
    "\n",
    "dt_train_score=print_Score(dt_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)\n",
    "dt_test_model=print_Score(dt_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## KNN\n",
    "\n",
    "knn=KNeighborsClassifier()\n",
    "knn_model=knn.fit(X_train_new,Y_train_resample_flat)\n",
    "\n",
    "knn_train_score=print_Score(knn_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=True)\n",
    "knn_test_score=print_Score(knn_model,X_train_new,X_test_new,Y_train_resample_flat,Y_test_resample_flat,train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA\n",
    "Using Pandas Profiling for EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile=ProfileReport(data,title=\"Thyroid Disease Detection\",explorative=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save EDA report to html file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_file(\"EDA Report.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
